{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pre_Embedding.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XFXg-AQqBlRl","colab_type":"code","colab":{}},"source":["import csv\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkaIem_s_SU0","colab_type":"code","outputId":"2ffd844e-b93b-4d4a-d0ee-42c2da97dc1e","executionInfo":{"status":"ok","timestamp":1571495207980,"user_tz":-420,"elapsed":3449,"user":{"displayName":"Dương Phạm","photoUrl":"","userId":"13675665718232856610"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","np.random.seed(0)\n","from keras.models import load_model, Model\n","from keras.layers import  Input, Dropout, LSTM, Activation, RepeatVector, Lambda,Dense\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.initializers import glorot_uniform\n","from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","np.random.seed(1)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vbO6YpETJwc8","colab_type":"text"},"source":["# **Pre-Progressing Data**"]},{"cell_type":"code","metadata":{"id":"oT2lIVPh_iLE","colab_type":"code","colab":{}},"source":["\n","def read_glove_vecs(glove_file):\n","    with open(glove_file, 'r',encoding='UTF-8') as f:\n","        words = set()\n","        word_to_vec_map = {}\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","        \n","        i = 1\n","        words_to_index = {}\n","        index_to_words = {}\n","        for w in sorted(words):\n","            words_to_index[w] = i\n","            index_to_words[i] = w\n","            i = i + 1\n","\n","    m = len(words_to_index)\n","    count=0\n","    for i in range(1,m+1):\n","      word=index_to_words[i]\n","      if word_to_vec_map[word].shape[0]!=50:\n","        word_to_vec_map.pop(word)\n","        words_to_index.pop(word)\n","        count+=1\n","    return words_to_index, index_to_words, word_to_vec_map,count"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XEDkD5fLGuO","colab_type":"code","colab":{}},"source":["def sentence_to_embed(sentence,word_to_vec_map):\n","  words=[x.lower() for x in sentence.split()]\n","  idx=[]\n","  for w in words:\n","    idx.append(word_to_vec_map[w])\n","  return idx\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pbpL6Do-Pun","colab_type":"code","colab":{}},"source":["def sentence_to_index(X,max_len,word_to_index):\n","  lenght=X.shape[0]\n","  X_zeros=np.zeros((lenght,max_len))\n","\n","  for i in range(lenght):\n","    words = [w.lower() for w in X[i].split()]\n","\n","    for j,w in enumerate(words[:max_len]):\n","      try:\n","        X_zeros[i,j]= word_to_index[w]\n","      except:\n","        X_zeros[i,j]= 1999\n","\n","  return X_zeros\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I1_Pa9TMBIjq","colab_type":"text"},"source":["# **Embedding Layer**"]},{"cell_type":"code","metadata":{"id":"PQqYs83hBHto","colab_type":"code","colab":{}},"source":["def Embedding_layer(word_to_vec_map, word_to_index,count):\n","\n","    \n","    vocab_len = len(word_to_index) + 1+count                 # adding 1 to fit Keras embedding (requirement)\n","    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]    # define dimensionality of your GloVe word vectors (= 50)\n","    \n","    ### START CODE HERE ###\n","    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n","    emb_matrix = np.zeros((vocab_len, emb_dim))\n","    \n","    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n","    for word, index in word_to_index.items():\n","        emb_matrix[index, :] = word_to_vec_map[word].reshape(50,)\n","\n","    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n","    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n","    ### END CODE HERE ###\n","\n","    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n","    embedding_layer.build((None,))\n","    \n","    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n","    embedding_layer.set_weights([emb_matrix])\n","    \n","    return embedding_layer\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGVemU44P9WV","colab_type":"text"},"source":["# **Attention Machanism**"]},{"cell_type":"code","metadata":{"id":"hj66_Ak4P8wE","colab_type":"code","colab":{}},"source":["def attention(s_prev,a,max_document_length):\n","  s_prev = RepeatVector(max_document_length)\n","  concat = Concatenate([a,s_prev],axis=-1)\n","  alpha_scores = Dense(1,activation='relu')(concat)\n","  alpha = Activation('softmax',axis=-1)(alpha_scores)\n","  context = Dot(axes = 1)([a,alpha])\n","  return context\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ez2lcghaVltz","colab_type":"code","colab":{}},"source":["def Sentiment_Model(input_shape,n_a,n_s,word_to_idex,word_to_vec_map,attention):\n","\n","  sentence_index=Input(input_shape,dtype='int32')\n","  s0 = Input(shape=(n_s,), name='s0')\n","  c0 = Input(shape=(n_s,), name='c0')\n","  s = s0\n","  c = c0\n","  embedding_layer= Embedding_layer(word_to_vec_map, word_to_index,count)\n","\n","  # Start model:\n","  embedding= embedding_layer(sentence_index)\n","  a = Bidirectional(LSTM(n_a, return_sequences=True))(embedding)\n","  x=Conv1D(filters,kernel_size,padding='valid',activation='relu',strides=1)(embedding)\n","  x=MaxPooling1D(pool_size=pool_size)(x)\n","  x=LSTM(lstm_output_size)(x)\n","  x=Dense(1)(x)\n","  x=Activation('sigmoid')(x)  \n","    # Create Model instance which converts sentence_indices into X.\n","  model = Model(inputs=sentence_index, outputs=x)\n","    \n","    ### END CODE HERE ###\n","    \n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Opm1GZ9_zKFB","colab_type":"text"},"source":["# **Prediction**"]},{"cell_type":"code","metadata":{"id":"945eZNoizJRh","colab_type":"code","colab":{}},"source":["def predict_sentence(model,sentence,word_to_index,max_document_length,thresold=0.5):\n","  index_sentence= sentence_to_index(sentence,max_document_length,word_to_index)\n","  pred = model.predict(index_sentence)\n","\n","  sentiment=[]\n","  for x in pred:\n","    if 0<=x<thresold:\n","      sentiment.append('NEGATIVE')\n","    elif thresold<=x<=1:\n","      sentiment.append('POSITIVE')\n","  for i in range(sentence.shape[0]):\n","      print('Sentence: '+ sentence[i] + ' is our model predicted as '+ sentiment[i])\n","  return sentiment\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bM8Vdag550P","colab_type":"text"},"source":["# **Visualization**"]},{"cell_type":"code","metadata":{"id":"qMOhNK6q55Pt","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","def visualize(history):\n","  acc = history.history['acc']\n","  val_acc = history.history['val_acc']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  epochs = range(len(acc))\n","\n","  plt.plot(epochs, acc, 'r', label='Training accuracy')\n","  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","  plt.title('Training and validation accuracy')\n","\n","  plt.figure()\n","\n","  plt.plot(epochs, loss, 'r', label='Training Loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YDyU9ZG-5BG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}